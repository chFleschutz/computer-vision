{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b695ec4-d0f4-402a-bfe6-7d26af086989",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f70ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab1cc5-6e04-4d11-a717-6a7d89086403",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05dd256",
   "metadata": {},
   "source": [
    "Download dataset from [here](https://data.caltech.edu/records/65de6-vp158/files/CUB_200_2011.tgz?download=1) and extract it into the `data` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8d0dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lr\": 1e-3,\n",
    "    \"batch_size\": 16,\n",
    "    \"image_dir\": \"data/CUB_200_2011/images\",\n",
    "    \"segmentation_dir\": \"data/CUB_200_2011/segmentations\",\n",
    "    \"image_paths\": \"data/CUB_200_2011/images.txt\",\n",
    "    \"epochs\": 10,\n",
    "    \"checkpoint\": \"checkpoint/bird_segmentation_v1.pth\",\n",
    "    \"optimiser\": \"checkpoint/bird_segmentation_v1_optim.pth\",\n",
    "    \"continue_train\": False,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12b7aa86-f0f1-4fee-8d7d-017c61a0d57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory checkpoint already exists!\n",
      "Directory test/pred already exists!\n",
      "Directory test/true already exists!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def makeSubDir(subdir):\n",
    "    if not os.path.exists(subdir):\n",
    "        os.makedirs(subdir)\n",
    "        print(\"Directory\", subdir, \"created successfully!\")\n",
    "    else:\n",
    "        print(\"Directory\", subdir, \"already exists!\")\n",
    "\n",
    "makeSubDir(\"checkpoint\")\n",
    "makeSubDir(\"test/pred\")\n",
    "makeSubDir(\"test/true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7b89be",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd20b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdDataset(Dataset):\n",
    "    def __init__(self, image_paths, image_dir, segmentation_dir, transform_image, transform_mask):\n",
    "        super(BirdDataset, self).__init__()\n",
    "        self.image_dir = image_dir\n",
    "        self.segmentation_dir = segmentation_dir\n",
    "        self.transform_image = transform_image\n",
    "        self.transform_mask = transform_mask\n",
    "        with open(image_paths, 'r') as f:\n",
    "            self.images_paths = [line.split(\" \")[-1] for line in f.readlines()]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_name = \".\".join(self.images_paths[index].split('.')[:-1])\n",
    "\n",
    "        image = Image.open(os.path.join(self.image_dir, f\"{image_name}.jpg\")).convert(\"RGB\")\n",
    "        seg = Image.open(os.path.join(self.segmentation_dir, f\"{image_name}.png\")).convert(\"L\")\n",
    "\n",
    "        image = self.transform_image(image)\n",
    "        seg = self.transform_mask(seg)\n",
    "\n",
    "        return image, seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cf6edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_image = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0., 0., 0.), (1., 1., 1.))\n",
    "])\n",
    "\n",
    "transforms_mask = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.,), (1.,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6016f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_set(image_paths, image_dir, segmentation_dir, transforms, batch_size=8, shuffle=True):\n",
    "    dataset = BirdDataset(image_paths,\n",
    "                          image_dir,\n",
    "                          segmentation_dir,\n",
    "                          transform_image=transforms[0],\n",
    "                          transform_mask=transforms[1])\n",
    "    print(\"Complete Dataset length: \", len(dataset))\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [len(dataset)-16, 16])\n",
    "\n",
    "    return DataLoader( train_dataset, batch_size=batch_size, shuffle=shuffle), \\\n",
    "           DataLoader( val_dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "990f3db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Dataset length:  11788\n",
      "loaded 736 batches\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset = load_data_set(\n",
    "    config['image_paths'],\n",
    "    config['image_dir'],\n",
    "    config['segmentation_dir'],\n",
    "    transforms=[transforms_image, transforms_mask],\n",
    "    batch_size=config['batch_size']\n",
    ")\n",
    "\n",
    "print(\"loaded\", len(train_dataset), \"batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f02d5bf",
   "metadata": {},
   "source": [
    "# UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14fa5e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(conv_block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7ef05d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(encoder_block, self).__init__()\n",
    "        self.conv = conv_block(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.pool(x), x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d728f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(decoder_block, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_channels + out_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x, bridge):\n",
    "        x = self.up(x)\n",
    "\n",
    "        if x.shape != bridge.shape:\n",
    "            x = TF.resize(x, size=bridge.shape[2:])\n",
    "    \n",
    "        x = torch.cat([x, bridge], dim=1)\n",
    "        return self.conv(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "109dfe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder1 = encoder_block(in_channels, 64)\n",
    "        self.encoder2 = encoder_block(64, 128)\n",
    "        self.encoder3 = encoder_block(128, 256)\n",
    "        self.encoder4 = encoder_block(256, 512)\n",
    "\n",
    "        self.center = conv_block(512, 1024)\n",
    "        \n",
    "        self.decoder4 = decoder_block(1024, 512)\n",
    "        self.decoder3 = decoder_block(512, 256)\n",
    "        self.decoder2 = decoder_block(256, 128)\n",
    "        self.decoder1 = decoder_block(128, 64)\n",
    "\n",
    "        self.output = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, bridge1 = self.encoder1(x)\n",
    "        x, bridge2 = self.encoder2(x)\n",
    "        x, bridge3 = self.encoder3(x)\n",
    "        x, bridge4 = self.encoder4(x)\n",
    "\n",
    "        x = self.center(x)\n",
    "\n",
    "        x = self.decoder4(x, bridge4)\n",
    "        x = self.decoder3(x, bridge3)\n",
    "        x = self.decoder2(x, bridge2)\n",
    "        x = self.decoder1(x, bridge1)\n",
    "\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9811b3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((3, 3, 256, 256))\n",
    "model = UNet(3, 1)\n",
    "preds = model(x)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd28a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(3, 1).to(config['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
